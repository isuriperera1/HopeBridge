{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMDTSDPiUCBnVIUx97nXJ8q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/isuriperera1/HopeBridge/blob/Face-Recognition/FaceRecognize_trials.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7d5gbwSk8PDp"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "\n",
        "# Load the pre-trained model\n",
        "model_path = 'face_model.h5'\n",
        "model = load_model(model_path)\n",
        "\n",
        "# Display the model summary to verify input shape\n",
        "model.summary()\n",
        "\n",
        "# Load and preprocess an image\n",
        "def preprocess_image(image_path, target_size):\n",
        "    # Ensure the input matches the model's expected shape\n",
        "    image = load_img(image_path, target_size=target_size, color_mode='rgb')  # Change 'grayscale' to 'rgb' if required\n",
        "    image_array = img_to_array(image)\n",
        "    image_array = np.expand_dims(image_array, axis=0)  # Add batch dimension\n",
        "    image_array /= 255.0  # Normalize the image\n",
        "    return image_array\n",
        "\n",
        "# Example: Provide the path to your test image\n",
        "image_path = 'image.jpeg'  # Replace with your image path\n",
        "\n",
        "# Adjust the target size as per the model's input\n",
        "target_size = (224, 224)  # Update based on the model input shape\n",
        "image = preprocess_image(test_image_path, target_size=target_size)\n",
        "\n",
        "# Ensure compatibility with model input\n",
        "predictions = model.predict(image)\n",
        "print(\"Predictions:\", predictions)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model directly\n",
        "from transformers import AutoImageProcessor, AutoModel\n",
        "\n",
        "processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
        "model = AutoModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
        "model = load_model(model_path)\n",
        "\n",
        "# Define emotion labels\n",
        "emotion_labels = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
        "\n",
        "def preprocess_image(image_path, target_size):\n",
        "    image = load_img(image_path, target_size=target_size)  # Resize the image\n",
        "    image_array = img_to_array(image)\n",
        "    image_array = cv2.cvtColor(image_array, cv2.COLOR_RGB2GRAY)  # Convert to grayscale\n",
        "    image_array = np.expand_dims(image_array, axis=-1)  # Add channel dimension\n",
        "    image_array = np.expand_dims(image_array, axis=0)  # Add batch dimension\n",
        "    image_array /= 255.0  # Normalize the image\n",
        "    return image_array\n",
        "\n",
        "# Example: Provide the path to your test image\n",
        "test_image_path = 'image.jpeg'  # Replace with your image path\n",
        "image = preprocess_image(test_image_path, target_size=(48, 48))  # Resize to 48x48\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(image)[0]  # Get the first prediction from the batch\n",
        "\n",
        "# Get the emotion with the highest probability\n",
        "predicted_emotion = emotion_labels[np.argmax(predictions)]\n",
        "\n",
        "# Display the result\n",
        "print(predicted_emotion)\n"
      ],
      "metadata": {
        "id": "TKk5P8nL8dyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install roboflow"
      ],
      "metadata": {
        "id": "VwRz0gLj8u3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from inference_sdk import InferenceHTTPClient\n",
        "\n",
        "CLIENT = InferenceHTTPClient(\n",
        "    api_url=\"https://detect.roboflow.com\",\n",
        "    api_key=\"API_KEY\"\n",
        ")\n",
        "\n",
        "result = CLIENT.infer(image.jpg, model_id=\"facial-emotion-recognition/2\")"
      ],
      "metadata": {
        "id": "zLPZ7JF-8yOc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}