# -*- coding: utf-8 -*-
"""Bert_Suicide_Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GNV1tKB7eeEWcOwJoCwDOKB5lsl-ndLK
"""

!pip install datasets

import tensorflow as tf
from transformers import TFAutoModel, AutoTokenizer
from datasets import load_dataset
from collections import Counter

model = TFAutoModel.from_pretrained("bert-base-uncased")

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

inputs = tokenizer(['Hello', 'How are you'], padding=True, truncation=True, return_tensors='tf')
inputs

output = model(inputs)
output

dataset = load_dataset('jquiros/suicide')

# Preprocess dataset: Drop the first column, keep second as text, and third as label
def preprocess_data(example):
    return {"text": example["text"], "label": 1 if example["class"] == "suicide" else 0}

dataset = dataset.map(preprocess_data)
dataset = dataset.remove_columns(["Unnamed: 0", "class"])  # Remove unnecessary columns

# Splitting dataset
dataset = dataset["train"].train_test_split(test_size=0.2)

dataset

print("Sample Entries from the Training Dataset:")
print(dataset['train'][:5])

def tokenize(batch): return tokenizer(batch['text'], padding=True, truncation=True, max_length=128)

dataset_encoded = dataset.map(tokenize, batched=True, batch_size=None)

dataset_encoded

dataset_encoded.set_format('tf', columns=['input_ids', 'attention_mask', 'token_type_ids', 'label'])


BATCH_SIZE = 64

def order(inp):
    data = list(inp.values())
    return {
        'input_ids': data[1],
        'attention_mask': data[2],
        'token_type_ids': data[3]
    }, data[0]

# converting train split of `dataset_encoded` to tensorflow format
train_dataset = tf.data.Dataset.from_tensor_slices(dataset_encoded['train'][:])
# set batch_size and shuffle
train_dataset = train_dataset.batch(BATCH_SIZE).shuffle(1000)
# map the `order` function
train_dataset = train_dataset.map(order, num_parallel_calls=tf.data.AUTOTUNE)

# doing the same for test set
test_dataset = tf.data.Dataset.from_tensor_slices(dataset_encoded['test'][:])
test_dataset = test_dataset.batch(BATCH_SIZE)
test_dataset = test_dataset.map(order, num_parallel_calls=tf.data.AUTOTUNE)

inp, out = next(iter(train_dataset))
print(inp, '\n\n', out)

class BERTForClassification(tf.keras.Model):
    def __init__(self, bert_model, num_classes):
        super().__init__()
        self.bert = bert_model
        self.fc = tf.keras.layers.Dense(num_classes, activation='softmax')

    def call(self, inputs):
        x = self.bert(inputs)[1]
        return self.fc(x)

    def get_config(self):
        config = super().get_config()
        config.update({
            "num_classes": self.fc.units,  # Store number of output classes
        })
        return config

    @classmethod
    def from_config(cls, config):
        from transformers import TFBertModel  # Import BERT model
        bert_model = TFBertModel.from_pretrained("bert-base-uncased")  # Load base model again
        return cls(bert_model, config["num_classes"])

classifier = BERTForClassification(model, num_classes=2)

classifier.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),
    loss=tf.keras.losses.SparseCategoricalCrossentropy(),
    metrics=['accuracy']
)

history = classifier.fit(
    train_dataset,
    epochs=2
)

classifier.evaluate(test_dataset)

import tensorflow as tf

def test_suicide_detection(texts, classifier, tokenizer):
    """
    Predicts whether input texts indicate suicidal thoughts or not.

    Args:
        texts (list): List of text strings to classify.
        classifier: Trained BERT classifier model.
        tokenizer: Tokenizer used during training.

    Returns:
        None. Prints out the text and its predicted category (suicide/non-suicide).
    """
    label_mapping = {0: "non-suicide", 1: "suicide"}

    # Tokenize input text
    inputs = tokenizer(texts, padding=True, truncation=True, max_length=128, return_tensors="tf")

    # Get predictions
    outputs = classifier(inputs)
    predictions = tf.argmax(outputs, axis=1).numpy()

    # Map predictions to labels
    predicted_labels = [label_mapping[pred] for pred in predictions]

    # Print results
    print("\nPredicted Suicide Classification:")
    for text, label in zip(texts, predicted_labels):
        print(f"Text: '{text}' => Predicted Category: {label}")

# Example texts to classify
test_texts = [
    "I feel so hopeless and lost, life is meaningless.",
    "I am really happy and excited about my upcoming vacation!",
    "Nothing seems to work out for me. I don't know what to do anymore.",
    "what a great day today is. We won the game and Im so happy",
    "I think Im doing really good these days",
    "I dont want to live anymore",
    "Nobody loves me",
    "I want to kill my self",
    "I am starting to feel great",
    "I am going to shoot my self today I cant do this anymore",
    "I went on an amazing date today, she was really nice",
    "I am going through a hard breakup and middle of that Im financially broken. I feel like the only option is to take my life",
    "I hate my life so much",
    "Today is the best day of my life because i got graduated",
    "yoo I cant do this shit anymore Im tired",
    "today game was fireee.. damn what a match"
]

# Run the test
test_suicide_detection(test_texts, classifier, tokenizer)

import tensorflow as tf
import shutil
from google.colab import files

# Define save paths
model_save_path = "bert_suicide_model"
tokenizer_save_path = "tokenizer"

# Save the model
classifier.save(model_save_path)

# Save the tokenizer
tokenizer.save_pretrained(tokenizer_save_path)

# Zip the model and tokenizer for download
shutil.make_archive(model_save_path, 'zip', model_save_path)
shutil.make_archive(tokenizer_save_path, 'zip', tokenizer_save_path)

# Download model and tokenizer
files.download(model_save_path + ".zip")
files.download(tokenizer_save_path + ".zip")